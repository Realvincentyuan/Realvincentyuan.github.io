<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Decision Tree Learning Notes</title>
    <link rel="stylesheet" href="assets/built/screen.css%3Fv=e2e91b37b9.css">


    <style>
        :root {
            --background-color: #ffffff
        }
    </style>

    <script>
        /* The script for calculating the color contrast was taken from
        https://gomakethings.com/dynamically-changing-the-text-color-based-on-background-color-contrast-with-vanilla-js/ */
        var accentColor = getComputedStyle(document.documentElement).getPropertyValue('--background-color');
        accentColor = accentColor.trim().slice(1);
        var r = parseInt(accentColor.substr(0, 2), 16);
        var g = parseInt(accentColor.substr(2, 2), 16);
        var b = parseInt(accentColor.substr(4, 2), 16);
        var yiq = ((r * 299) + (g * 587) + (b * 114)) / 1000;
        var textColor = (yiq >= 128) ? 'dark' : 'light';

        document.documentElement.className = `has-${textColor}-text`;
    </script>

    <link rel="canonical" href="decision-tree-learning-notes.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Spacecraft">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Decision Tree Learning Notes">
    <meta property="og:description" content="Decision Tree Learning Notes


1 Introduction

Decision trees are very classical machine learning models that can be used to solve many classification and regression problems in daily work. Many more advanced machine learning models are also built based on decision trees. Today, let&#x27;s review some of the most important technical">
    <meta property="og:url" content="https://realvincentyuan.github.io/Spacecraft/decision-tree-learning-notes/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta property="article:published_time" content="2021-08-28T05:00:00.000Z">
    <meta property="article:modified_time" content="2023-07-02T03:36:38.000Z">
    <meta property="article:tag" content="Tech">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="AI">
    
    <meta property="article:publisher" content="https://www.facebook.com/ghost">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Decision Tree Learning Notes">
    <meta name="twitter:description" content="Decision Tree Learning Notes


1 Introduction

Decision trees are very classical machine learning models that can be used to solve many classification and regression problems in daily work. Many more advanced machine learning models are also built based on decision trees. Today, let&#x27;s review some of the most important technical">
    <meta name="twitter:url" content="https://realvincentyuan.github.io/Spacecraft/decision-tree-learning-notes/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Vincent Yuan">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Tech, Python, AI">
    <meta name="twitter:site" content="@ghost">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="1333">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Spacecraft",
        "url": "https://realvincentyuan.github.io/Spacecraft/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://realvincentyuan.github.io/Spacecraft/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Vincent Yuan",
        "url": "https://realvincentyuan.github.io/Spacecraft/author/vincent/",
        "sameAs": []
    },
    "headline": "Decision Tree Learning Notes",
    "url": "https://realvincentyuan.github.io/Spacecraft/decision-tree-learning-notes/",
    "datePublished": "2021-08-28T05:00:00.000Z",
    "dateModified": "2023-07-02T03:36:38.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&ixlib=rb-4.0.3&q=80&w=2000",
        "width": 2000,
        "height": 1333
    },
    "keywords": "Tech, Python, AI",
    "description": "Decision Tree Learning Notes\n\n\n1 Introduction\n\nDecision trees are very classical machine learning models that can be used to solve many classification and regression problems in daily work. Many more advanced machine learning models are also built based on decision trees. Today, let&#x27;s review some of the most important technical details in decision trees to solidify the foundation and correctly apply decision trees.\n\n\n2 Important Algorithm Details\n\n\n2.1 Making Predictions\n\nThe example displays a ",
    "mainEntityOfPage": "https://realvincentyuan.github.io/Spacecraft/decision-tree-learning-notes/"
}
    </script>

    <meta name="generator" content="Ghost 5.49">
    <link rel="alternate" type="application/rss+xml" title="Spacecraft" href="rss/index.html">
    <script defer src="https://cdn.jsdelivr.net/ghost/portal@~2.33/umd/portal.min.js" data-i18n="false" data-ghost="https://realvincentyuan.github.io/Spacecraft/" data-key="206cb569cfe36163ae0b6ed398" data-api="https://realvincentyuan.github.io/Spacecraft/ghost/api/content/" crossorigin="anonymous"></script><style id="gh-members-styles">.gh-post-upgrade-cta-content,
.gh-post-upgrade-cta {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    text-align: center;
    width: 100%;
    color: #ffffff;
    font-size: 16px;
}

.gh-post-upgrade-cta-content {
    border-radius: 8px;
    padding: 40px 4vw;
}

.gh-post-upgrade-cta h2 {
    color: #ffffff;
    font-size: 28px;
    letter-spacing: -0.2px;
    margin: 0;
    padding: 0;
}

.gh-post-upgrade-cta p {
    margin: 20px 0 0;
    padding: 0;
}

.gh-post-upgrade-cta small {
    font-size: 16px;
    letter-spacing: -0.2px;
}

.gh-post-upgrade-cta a {
    color: #ffffff;
    cursor: pointer;
    font-weight: 500;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a:hover {
    color: #ffffff;
    opacity: 0.8;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a.gh-btn {
    display: block;
    background: #ffffff;
    text-decoration: none;
    margin: 28px 0 0;
    padding: 8px 18px;
    border-radius: 4px;
    font-size: 16px;
    font-weight: 600;
}

.gh-post-upgrade-cta a.gh-btn:hover {
    opacity: 0.92;
}</style>
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="206cb569cfe36163ae0b6ed398" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://realvincentyuan.github.io/Spacecraft/" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/ghost/announcement-bar@~1.1/umd/announcement-bar.min.js" data-announcement-bar="https://realvincentyuan.github.io/Spacecraft/" data-api-url="https://realvincentyuan.github.io/Spacecraft/members/api/announcement/" crossorigin="anonymous"></script>
    <link href="https://realvincentyuan.github.io/Spacecraft/webmentions/receive/" rel="webmention">
    <script defer src="public/cards.min.js%3Fv=e2e91b37b9"></script>
    <link rel="stylesheet" type="text/css" href="public/cards.min.css%3Fv=e2e91b37b9.css">
    <script defer src="public/member-attribution.min.js%3Fv=e2e91b37b9"></script>
    <!-- Reading progress bar -->
<style>
.reading-progress {
  position: fixed;
  top: 0;
  z-index: 999;
  width: 100%;
  height: 5px; /* Progress bar height */
  background: #c5d2d9; /* Progress bar background color */
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; /* Hide default progress bar */
}

.reading-progress::-webkit-progress-bar {
  background-color: transparent;
}

.reading-progress::-webkit-progress-value {
  background: var(--ghost-accent-color); /* Progress bar color */
}
</style><style>:root {--ghost-accent-color: #FF1A75;}</style>
</head>

<body class="post-template tag-tech tag-python tag-ai tag-hash-import-2023-07-07-20-42 is-head-left-logo has-classic-feed">
<div class="gh-site">

    <header id="gh-head" class="gh-head gh-outer">
        <div class="gh-head-inner gh-inner">
            <div class="gh-head-brand">
                <div class="gh-head-brand-wrapper">
                    <a class="gh-head-logo" href="index.html">
                            Spacecraft
                    </a>
                </div>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M17.5 17.5L12.5 12.5L17.5 17.5ZM14.1667 8.33333C14.1667 9.09938 14.0158 9.85792 13.7226 10.5657C13.4295 11.2734 12.9998 11.9164 12.4581 12.4581C11.9164 12.9998 11.2734 13.4295 10.5657 13.7226C9.85792 14.0158 9.09938 14.1667 8.33333 14.1667C7.56729 14.1667 6.80875 14.0158 6.10101 13.7226C5.39328 13.4295 4.75022 12.9998 4.20854 12.4581C3.66687 11.9164 3.23719 11.2734 2.94404 10.5657C2.65088 9.85792 2.5 9.09938 2.5 8.33333C2.5 6.78624 3.11458 5.30251 4.20854 4.20854C5.30251 3.11458 6.78624 2.5 8.33333 2.5C9.88043 2.5 11.3642 3.11458 12.4581 4.20854C13.5521 5.30251 14.1667 6.78624 14.1667 8.33333Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</button>
                <button class="gh-burger"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="index.html">Home</a></li>
    <li class="nav-tech"><a href="tag/tech/index.html">Tech</a></li>
    <li class="nav-profession"><a href="tag/pro/index.html">Profession</a></li>
    <li class="nav-about"><a href="about/index.html">About</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                    <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M17.5 17.5L12.5 12.5L17.5 17.5ZM14.1667 8.33333C14.1667 9.09938 14.0158 9.85792 13.7226 10.5657C13.4295 11.2734 12.9998 11.9164 12.4581 12.4581C11.9164 12.9998 11.2734 13.4295 10.5657 13.7226C9.85792 14.0158 9.09938 14.1667 8.33333 14.1667C7.56729 14.1667 6.80875 14.0158 6.10101 13.7226C5.39328 13.4295 4.75022 12.9998 4.20854 12.4581C3.66687 11.9164 3.23719 11.2734 2.94404 10.5657C2.65088 9.85792 2.5 9.09938 2.5 8.33333C2.5 6.78624 3.11458 5.30251 4.20854 4.20854C5.30251 3.11458 6.78624 2.5 8.33333 2.5C9.88043 2.5 11.3642 3.11458 12.4581 4.20854C13.5521 5.30251 14.1667 6.78624 14.1667 8.33333Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</button>
                    <div class="gh-head-members">
                                <a class="gh-head-link" href="decision-tree-learning-notes.html#/portal/signin" data-portal="signin">Sign in</a>
                                <a class="gh-head-btn gh-btn gh-primary-btn" href="decision-tree-learning-notes.html#/portal/signup" data-portal="signup">Subscribe</a>
                    </div>
            </div>
        </div>
    </header>

    
<progress class="reading-progress" value="0" max="100" aria-label="Reading progress"></progress>



<main class="gh-main gh-outer">
    <div class="gh-inner">
            <article class="gh-article post tag-tech tag-python tag-ai tag-hash-import-2023-07-07-20-42">
                    <header class="gh-article-header gh-canvas">
                        <h1 class="gh-article-title">Decision Tree Learning Notes</h1>
                            <figure class="gh-article-image has-caption">
        <img
            srcset="https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;300 300w,
                    https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;720 720w,
                    https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;960 960w,
                    https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1200 1200w,
                    https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000 2000w,
                    https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000"
            sizes="(max-width: 1200px) 100vw, 1200px"
            src="https://images.unsplash.com/photo-1596987851982-3b90e09ab4ac?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDQxfHxmb3Jlc3R8ZW58MHx8fHwxNjg4MjY4MjY4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1200"
            alt="Decision Tree Learning Notes"
        >
            <figcaption>Photo by <a href="https://unsplash.com/@cedric_photography?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Cédric VT</a> / <a href="https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Unsplash</a></figcaption>
    </figure>
                    </header>

                <section class="gh-content gh-canvas">
                    <h1 id="decision-tree-learning-notes">Decision Tree Learning Notes</h1><h2 id="1-introduction">1 Introduction</h2><p>Decision trees are very classical machine learning models that can be used to solve many classification and regression problems in daily work. Many more advanced machine learning models are also built based on decision trees. Today, let's review some of the most important technical details in decision trees to solidify the foundation and correctly apply decision trees.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/2021-8-28/1630120948129-iris_tree.png" class="kg-image" alt loading="lazy"><figcaption>Example of a decision tree for classifying iris flower species</figcaption></figure><h2 id="2-important-algorithm-details">2 Important Algorithm Details</h2><h3 id="21-making-predictions">2.1 Making Predictions</h3><p>The example displays a decision tree with a depth of 2, showing the process and conclusions of decision-making. For 150 sample points, at the root node, the decision tree divides the data into two parts based on whether the petal length is less than 2.45 cm. The samples with a petal length less than 2.45 cm are classified as setosa, and the ones with a petal length greater than 2.45 cm are further classified based on whether the petal width is less than 1.75 cm. The ones that are less than 1.75 cm are considered versicolor, and the ones that are greater than 1.75 cm are considered virginica.</p><p>The "samples" in the graph represent the number of samples in each category. For example, in the left leaf node at depth 1, "samples=50" means that there are 50 samples with a petal length less than 2.45 cm. The "value" represents the distribution of training data in the current node. For example, in the green left node at depth 2, "[0, 49, 5]" represents that there are 0 setosa, 49 versicolor, and 5 virginica in this node, totaling 54 samples.</p><h3 id="22-basis-for-predictions">2.2 Basis for Predictions</h3><p>In the decision tree example, there is another important indicator called "Gini impurity," which measures the impurity of the current node. Intuitively, when all the samples in a node belong to the same class, the purity of the node is the highest, and the Gini impurity is 0. The definition of "Gini" is as follows:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/2021-8-28/1630138292390-gini.png" class="kg-image" alt loading="lazy"><figcaption>Gini impurity</figcaption></figure><p>where Pi,k is the proportion of samples in class k in node i. <sup> </sup>In the commonly used Python machine learning library <code>Scikit-Learn (v0.24.2)</code>, when implementing the classification and regression tree (Classification and Regression Tree, CART), in the process of selecting split nodes, the basis for the decision tree to select split nodes and thresholds is related to . Its optimization objective (loss function) is as follows:<code>Gini</code></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://realvincentyuan.github.io/Spacecraft/content/images/2023/07/image-16.png" class="kg-image" alt loading="lazy" width="668" height="114" srcset="https://realvincentyuan.github.io/Spacecraft/content/images/size/w600/2023/07/image-16.png 600w, https://realvincentyuan.github.io/Spacecraft/content/images/2023/07/image-16.png 668w"><figcaption>Loss function</figcaption></figure><p><code>Gini</code> in The CART algorithm will do a greedy search (Greedy Search), start splitting from the root node, and search for features and thresholds that can be effectively reduced in the layer-by-layer child nodes , until the number of split layers reaches the maximum depth (defined by the max_depth parameter) or has been found Less than <code>Gini</code>the node that can be reduced. Intuitively, finding the best tree is an <a href="https://zh.wikipedia.org/wiki/NP%E5%AE%8C%E5%85%A8?ref=localhost">NP-complete</a> problem, so the algorithm will only find a relatively good solution in the end, not the best solution.</p><p>In addition to "Gini," entropy can also be used to measure the effectiveness of splitting nodes and to quantify the degree of disorder. In a decision tree, when all samples in a node belong to the same class, the entropy value is 0. The definition of entropy is as follows:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/2021-8-29/1630205324586-Entropy.png" class="kg-image" alt loading="lazy"><figcaption>Entropy</figcaption></figure><p>where Pi,k is the proportion of samples in class k in node i. In <code>Scikit-Learn (v0.24.2)</code>, when using the <code>DecisionTreeClassifier</code> class, you can set the <code>criterion</code> parameter to <code>entropy</code> to use entropy as the measure. However, the difference between using Gini and entropy is usually not significant. The main difference is that the Gini calculation is faster, and using Gini will make the tree concentrate the samples more in the nodes, while using entropy will make the distribution of samples in the tree more balanced.</p><h3 id="23-preventing-overfitting">2.3 Preventing Overfitting</h3><p>Decision trees themselves have almost no assumptions and do not rely on feature scaling, but the model itself needs constraints to prevent overfitting. Regularization can be achieved by controlling the model parameters. Taking the <code>DecisionTreeClassifier</code> class in <code>Scikit-Learn (v0.24.2)</code> as an example, the following parameters are commonly used for regularization to prevent overfitting:</p><ul><li><strong>max_depth</strong>: The maximum depth of the tree. The default value is <code>None</code>, which means there is no maximum depth limit for the tree.</li><li><strong>min_samples_split</strong>: The minimum number of samples required to split an internal node. The default value is 2.</li><li><strong>min_samples_leaf</strong>: The minimum number of samples required to be at a leaf node. The default value is 1.</li><li><strong>min_weight_fraction_leaf</strong>: The minimum weighted fraction of the total number of samples required to be at a leaf node. The default value is 0. When <code>class_weight</code> is set and the sample weights are different, this parameter constrains the weighted proportion of samples in the leaf nodes, similar to <code>min_samples_leaf</code>, but expressed as a proportion.</li><li><strong>max_features</strong>: The number of features to consider when looking for the best split. The default is to consider all features. Note that the decision tree will not stop searching for a valid split until it has searched the number of features specified by <code>max_features</code>, even if it exceeds that value.</li><li><strong>max_leaf_nodes</strong>: The maximum number of leaf nodes. The default value is <code>None</code>.</li><li><strong>min_impurity_decrease</strong>: The minimum impurity decrease required to split a node. The default value is 0.</li></ul><p>Usually, increasing the <code>min_</code> parameters or decreasing the <code>max_</code> parameters helps with regularization of the decision tree.</p><h3 id="24-regression-task">2.4 Regression Task</h3><p>In <code>Scikit-Learn (v0.24.2)</code>, you can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?ref=localhost#sklearn.tree.DecisionTreeRegressor">DecisionTreeRegressor</a> class to perform regression tasks.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/2021-8-29/1630228541305-iris_reg_tree.png" class="kg-image" alt="Regression Tree" loading="lazy"><figcaption><em>Regression tree</em></figcaption></figure><p>In regression tasks, the predicted value at a leaf node is the mean of the target values of the samples in that leaf node. The implementation of the CART algorithm for regression is similar to classification, but the optimization objective is to minimize the mean squared error (MSE) between the predicted values and the target values.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.jsdelivr.net/gh/BulletTech2021/Pics/2021-8-29/1630229474823-CART_regression_loss.png" class="kg-image" alt="Regression Tree Loss Function" loading="lazy"><figcaption><em>Loss function for a regression tree</em></figcaption></figure><p>The model parameters for regression trees are similar to those for classification trees, and you can use similar techniques to prevent overfitting.</p><h3 id="25-other-important-attributes">2.5 Other Important Attributes</h3><p>In the <code>Scikit-Learn</code> implementation, the <code>feature_importances_</code> attribute of a decision tree can show the importance of features. It is based on the reduction in the criterion measure for each feature and returns normalized values. If you have features with a high number of different values (high cardinality features), it is recommended to use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html?ref=localhost#sklearn.inspection.permutation_importance">sklearn.inspection.permutation_importance</a>.</p><p>If you want to manually adjust the tree, such as changing the splitting threshold, you can use <a href="https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html?ref=localhost#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py">sklearn.tree._tree.Tree</a>.</p><h2 id="3-summary">3 Summary</h2><p>Decision trees perform well for both classification and regression tasks. However, they have limitations and weaknesses. For example, they are sensitive to the direction and volatility of the data. These issues cannot be perfectly addressed by a single tree. So, is there a better approach by using multiple trees? In the next discussion, we will talk about random forests!</p><p>Here is an example code:</p><pre><code class="language-python"># Dependencies
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import export_graphviz

import matplotlib.pylab as plt
import numpy as np


# Load sample data
iris = load_iris()
X = iris.data[:, :2]  # Select petal length and petal width as features
y = iris.target

# View data distribution
plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.scatter(X[y == 2, 0], X[y == 2, 1])
plt.show()

# Build a decision tree
tree_clf = DecisionTreeClassifier(criterion='entropy', max_depth=2)
tree_clf.fit(X, y)

# Export decision tree graph
export_graphviz(
    tree_clf,
    out_file="iris_tree.dot",
    feature_names=iris.feature_names[:2],
    class_names=iris.target_names,
    rounded=True,
    filled=True
)

# Decision boundary plotting function
def plot_decision_boundary(model, x):
    # Generate coordinate matrices for the

 grid points, resulting in two matrices
    M, N = 500, 500
    x0, x1 = np.meshgrid(np.linspace(x[:, 0].min(), x[:, 0].max(), M), np.linspace(x[:, 1].min(), x[:, 1].max(), N))
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    z = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])
    plt.pcolormesh(x0, x1, z, cmap=custom_cmap)

# Plot decision boundary
plot_decision_boundary(tree_clf, X)
plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.scatter(X[y == 2, 0], X[y == 2, 1])
plt.show()

# View feature importance
print(tree_clf.feature_importances_)
</code></pre>
                    <aside class="gh-article-meta">
                        <div class="gh-article-meta-inner">
                                <div class="gh-article-meta-wrapper">
                                <h4 class="gh-author-name">
                                    <a href="author/vincent/index.html">Vincent Yuan</a>
                                </h4>
                            <time class="gh-article-date" datetime="2021-08-28">Aug 28, 2021</time></div>
                            


                             <a class="gh-article-tag" href="tag/tech/index.html" style="--tag-color: ">Tech</a>
                             <a class="gh-article-tag" href="tag/python/index.html" style="--tag-color: ">Python</a>
                             <a class="gh-article-tag" href="tag/ai/index.html" style="--tag-color: ">AI</a>
                           

                        </div>
                    </aside>
                </section>


                <footer class="gh-article-footer gh-canvas">
                    <nav class="gh-navigation">
                        <div class="gh-navigation-previous">
                                <a class="gh-navigation-link" href="tips-for-common-operations-on-python-dictionaries/index.html">← Previous</a>
                        </div>

                        <div class="gh-navigation-middle"></div>

                        <div class="gh-navigation-next">
                                <a class="gh-navigation-link" href="overview-of-snowflake-architecture/index.html">Next →</a>
                        </div>
                    </nav>
                </footer>
            </article>
    </div>
</main>

    <footer class="gh-foot gh-outer">
        <div class="gh-foot-inner gh-inner">
            <nav class="gh-foot-menu">
                <ul class="nav">
    <li class="nav-sign-up"><a href="decision-tree-learning-notes.html#/portal/">Sign up</a></li>
</ul>

            </nav>

            <div class="gh-copyright">
                    Spacecraft © 2023. Powered by <a href="https://ghost.org/" target="_blank" rel="noopener">Ghost</a>
            </div>
        </div>
    </footer>

</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script src="assets/built/main.min.js%3Fv=e2e91b37b9"></script>

  <script>
    const progressBar = document.querySelector('.reading-progress');

    function updateProgress() {
      const totalHeight = document.body.clientHeight;
      const windowHeight = document.documentElement.clientHeight;
      const position = window.scrollY;
      const progress = position / (totalHeight - windowHeight) * 100;
      progressBar.setAttribute('value', progress);
      requestAnimationFrame(updateProgress);
    }

    requestAnimationFrame(updateProgress);
  </script>




</body>

</html>
